{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h1 align=\"center\"> Sequence-to-Sequence: Translation</h1>\n",
    "    <h3 align=\"center\">Deep Learning in Python (HamYad Lab.)</h3>\n",
    "    <h5 align=\"center\"><a href=\"http://www.snrazavi.ir\">Seyed Naser RAZAVI</a></h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/Seq2Seq-arch.png\" width=\"90%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine Translations\n",
    "- Chatbots\n",
    "- Question Answering\n",
    "- Intelligent Word Processors\n",
    "- Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "from utils import *\n",
    "from data_utils import *\n",
    "from train_utils import *\n",
    "from wv import WordVector\n",
    "\n",
    "# setup\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# debugger\n",
    "from IPython.core.debugger import Pdb\n",
    "pdb = Pdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A parallel corpus (CSV file) containg pairs of sentences seperated by `<TAB>`.\n",
    "- Every pair contains an English sentence and its corresponding translation in Persian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/en2fa_corpus.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang1 = 'en'\n",
    "lang2 = 'fa'\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "MAX_VOCAB = 30000\n",
    "MIN_COUNT = 3\n",
    "\n",
    "PAD, UNK, SOS, EOS = 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "- Vocabulary object is the responsible object for tokenization and **numericaliztion**.\n",
    "- There is one vocabulary object for each source and target languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \n",
    "    SPECIAL_TOKENS = {'<PAD>': PAD, '<UNK>': UNK, '<SOS>': SOS, '<EOS>': EOS}\n",
    "    \n",
    "    def __init__(self, name, counter, min_count=3, max_vocab=30000):\n",
    "        self.lang_name = name        \n",
    "        self.word2count = OrderedDict(\n",
    "            [(w, c) for (w, c) in counter.most_common(max_vocab) if c >= min_count])\n",
    "        self.word2index = dict([(w, i+4) for  i, (w, _) in enumerate(self.word2count.items())])\n",
    "        self.index2word = dict([(i+4, w) for  i, (w, _) in enumerate(self.word2count.items())])\n",
    "        \n",
    "        for word, index in self.SPECIAL_TOKENS.items():\n",
    "            self.word2index[word] = index\n",
    "            self.index2word[index] = word\n",
    "            \n",
    "    def wtoi(self, word):\n",
    "        return self.word2index.get(word, self.SPECIAL_TOKENS['<UNK>'])\n",
    "        \n",
    "    def itow(self, index):\n",
    "        return self.index2word.get(index, -1)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            return self.itow(key)\n",
    "        else:\n",
    "            return self.wtoi(key)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(data_dir, lang1=\"en\", lang2=\"fa\", min_count=3, max_vocab=30000, reverse=False):\n",
    "    sentence_pairs = load_corpus(data_dir, lang1, lang2, reverse)   \n",
    "    sentence_pairs = filter_pairs(sentence_pairs, MAX_LENGTH)\n",
    "    print(\"{} sentence pairs selected.\".format(len(sentence_pairs)))\n",
    "    \n",
    "    print(\"\\nBuilding vocabularies for source and target language...\")\n",
    "    src_counter, tgt_counter = Counter(), Counter()\n",
    "    for (src_sent, tgt_sent) in sentence_pairs:\n",
    "        src_counter.update(src_sent.split(' '))\n",
    "        tgt_counter.update(tgt_sent.split(' '))\n",
    "        \n",
    "    if reverse:\n",
    "        lang1, lang2 = lang2, lang1\n",
    "        \n",
    "    src_vocab = Vocabulary(lang1, src_counter, min_count, max_vocab)\n",
    "    tgt_vocab = Vocabulary(lang2, tgt_counter, min_count, max_vocab)\n",
    "            \n",
    "    print(\"Number of words in each language:\")\n",
    "    print(\" - [{}]: {}\".format(src_vocab.lang_name, len(src_vocab)))\n",
    "    print(\" - [{}]: {}\".format(tgt_vocab.lang_name, len(tgt_vocab)))\n",
    "    \n",
    "    return src_vocab, tgt_vocab, sentence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab, tgt_vocab, sentence_pairs = prepare_data('data', lang1, lang2, MIN_COUNT, MAX_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sent, tgt_sent = random.choice(sentence_pairs)\n",
    "print(src_sent)\n",
    "print(tgt_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sentence_pairs, val_sentence_pairs = split(sentence_pairs, split_ratio=0.2)\n",
    "print(len(trn_sentence_pairs), len(val_sentence_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Contains source and target sentences\n",
    "- Provides `(x, y)` pairs for `Dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sentence_pairs, src_vocab, tgt_vocab):\n",
    "        \n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        \n",
    "        # Numerialize all sentences in both src and tgt languages\n",
    "        self.src_ids = [self.encode(x, src_vocab) for x, _ in sentence_pairs]\n",
    "        self.tgt_ids = [self.encode(y, tgt_vocab) for _, y in sentence_pairs]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"returns an (x, y) pair\"\n",
    "        x = self.src_ids[index]\n",
    "        y = self.tgt_ids[index]\n",
    "        return x, y\n",
    "    \n",
    "    def encode(self, sentence, vocab):\n",
    "        \"Converts an input sentence to token ids.\"\n",
    "        ids = [vocab.wtoi(token) for token in sentence.split(' ')] + [EOS]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, encoded_sentence, vocab):\n",
    "        \"Convert back from token ids to the original sentence.\"\n",
    "        return ' '.join([vocab.itow(i) for i in encoded_sentence[:-1]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Seq2SeqDataset(trn_sentence_pairs, src_vocab, tgt_vocab)\n",
    "valid_ds = Seq2SeqDataset(val_sentence_pairs, src_vocab, tgt_vocab)\n",
    "\n",
    "print(len(train_ds), len(valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# select a random (x, y) pair from training data\n",
    "idx = random.randint(0, len(train_ds) - 1)\n",
    "x, y = train_ds[idx]\n",
    "\n",
    "# print source sentence and its tensor\n",
    "print(x)\n",
    "print(train_ds.decode(x, src_vocab))\n",
    "print()\n",
    "\n",
    "# print target sentence and its tensor\n",
    "print(y)\n",
    "print(train_ds.decode(y, tgt_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we have implemented a custom function named `collate_fn()`.\n",
    "- The job of this function is to merge a list of samples into to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, Y, lengths = next(iter(train_dl))\n",
    "print(X.size())\n",
    "print(Y.size())\n",
    "print(lengths[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained word wectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_wvecs_filename = f'data/wiki.{lang1}.pkl'\n",
    "tgt_wvecs_filename = f'data/wiki.{lang2}.pkl'\n",
    "\n",
    "src_wvecs = WordVector(lang1, src_wvecs_filename)\n",
    "tgt_wvecs = WordVector(lang2, tgt_wvecs_filename)\n",
    "\n",
    "print(len(src_wvecs), len(tgt_wvecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings(vocab, wv):\n",
    "    num_vocabs, embed_sz = len(vocab), wv.vector_size\n",
    "    emb = torch.zeros(num_vocabs, embed_sz)\n",
    "    for idx in tqdm_notebook(range(num_vocabs)):\n",
    "        emb[idx] = torch.from_numpy(wv[vocab[idx]])\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embeddings = get_embeddings(src_vocab, src_wvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_embeddings = get_embeddings(tgt_vocab, tgt_wvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = src_vocab['the']\n",
    "wv1 = src_embeddings[idx]\n",
    "wv2 = torch.from_numpy(src_wvecs['the']).float()\n",
    "print((wv1 == wv2).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence To Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/Seq2Seq-arch.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/enc.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, num_layers=1, \n",
    "                 ndir=2, dropout_emb=0, dropout_rnn=0, dropout_ctx=0, emb=None):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        assert hidden_size % 2 == 0, 'Encoder size must be divisible by 2.'\n",
    "        self.hidden_size = hidden_size // ndir\n",
    "        self.num_layers = num_layers\n",
    "        self.ndir = ndir\n",
    "        self.dropout_emb = dropout_emb\n",
    "        self.dropout_ctx = dropout_ctx\n",
    "        bidir = (ndir == 2)\n",
    "        \n",
    "        if dropout_emb > 0:\n",
    "            self.emb_dropout = nn.Dropout(dropout_emb)\n",
    "        if dropout_ctx > 0:\n",
    "            self.ctx_dropout = nn.Dropout(dropout_ctx)\n",
    "            \n",
    "        if emb is not None:\n",
    "            assert emb.size(0) == input_size, 'Invalid embeddings!'\n",
    "            self.embed_size = emb.size(1)        \n",
    "            \n",
    "        self.embedding = nn.Embedding(input_size, self.embed_size, padding_idx=0)\n",
    "        self.gru = nn.GRU(self.embed_size, self.hidden_size, num_layers, \n",
    "                          dropout=dropout_rnn, bidirectional=bidir)\n",
    "        \n",
    "        if emb is not None:\n",
    "            self.embedding.weight.data = emb\n",
    "    \n",
    "    def forward(self, inputs, src_lenghts, hidden):\n",
    "        mask = (inputs != 0).float()\n",
    "        \n",
    "        # Embedding\n",
    "        embed = self.embedding(inputs)\n",
    "        if self.dropout_emb > 0:\n",
    "            embed = self.emb_dropout(embed)\n",
    "        \n",
    "        # Recurrent layer\n",
    "        pack_embed = pack(embed, src_lenghts)  # pack padded sequence\n",
    "        output, hidden = self.gru(pack_embed, hidden)\n",
    "        output = unpack(output)[0]             # pad packed sequence\n",
    "        if self.dropout_ctx > 0:\n",
    "            output = self.ctx_dropout(output)\n",
    "            \n",
    "        return output, hidden, mask\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return to_var(torch.zeros(self.ndir * self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention:\n",
    "Focus on different part of the encoder's output for every step of the decoder's own ouput.\n",
    "1. Calculate a set of `attention weights`.\n",
    "2. Compute a weighted sum of encoder outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/attn-full.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating attention weights\n",
    "Attention weights are computed with another feed-forward layer, using:\n",
    "- hidden state of decoder, and\n",
    "- the decoder's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, context_size, attn_type='mlp'):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        assert attn_type in ['dot', 'mlp'], f'Unknown attention type {attn_type}'\n",
    "        \n",
    "        if attn_type == 'mlp':\n",
    "            self.mlp = nn.Linear(context_size, 1, bias=False)\n",
    "            self.forward = self.mlp_forward\n",
    "        else:\n",
    "            self.forward = self.dot_forward\n",
    "        \n",
    "        self.hid2ctx = nn.Linear(hidden_size,  context_size, bias=False)\n",
    "        self.ctx2ctx = nn.Linear(context_size, context_size, bias=False)\n",
    "        self.ctx2hid = nn.Linear(context_size, hidden_size,  bias=False)\n",
    "        \n",
    "    def dot_forward(self, hidden, context, mask):\n",
    "        \"\"\" Dot-Product attention.\n",
    "           Inputs:\n",
    "              - hidden: decoder current output (1, bs, hs)\n",
    "              - context: encoder outputs       (sl, bs, cs)\n",
    "        \"\"\"\n",
    "        context_ = self.ctx2ctx(context).permute(1, 2, 0)  # (sl, bs, cs) -> (bs, cs, sl)\n",
    "        hidden_  = self.hid2ctx(hidden).permute(1, 0, 2)   # (1,  bs, hs) -> (bs,  1, hs)\n",
    "        \n",
    "        # dot product\n",
    "        scores = F.softmax(torch.bmm(hidden_, context_), dim=-1)  # (bs, 1, sl)\n",
    "        output = self.ctx2hid(torch.bmm(scores, context.transpose(0, 1)))   # (bs, 1, hs)\n",
    "        \n",
    "        return scores.transpose(0, 1), output.transpose(0, 1)\n",
    "\n",
    "    def mlp_forward(self, hidden, context, mask):\n",
    "        \"\"\" Dot-Product attention.\n",
    "           Inputs:\n",
    "              - hidden: decoder current output (1,  bs, hs)\n",
    "              - context: encoder outputs       (sl, bs, cs)\n",
    "        \"\"\"\n",
    "        context_ = self.ctx2ctx(context)\n",
    "        hidden_ = self.hid2ctx(hidden)\n",
    "        \n",
    "        # scores\n",
    "        scores = self.mlp(F.tanh(context_ + hidden_)).squeeze(-1)\n",
    "        \n",
    "        # normalize attention scores\n",
    "        alpha = (scores - scores.max(0)[0]).exp().mul(mask)\n",
    "        alpha = alpha / alpha.sum(0)\n",
    "        \n",
    "        output = self.ctx2hid((alpha.unsqueeze(-1) * context).sum(0))\n",
    "        \n",
    "        return alpha, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, output_size, context_size, num_layers=1, \n",
    "                 tie_weights=False, attn_type='mlp', dropout_out=0, emb=None):\n",
    "        \n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.context_size = context_size\n",
    "        self.tie_weights = tie_weights\n",
    "        self.attn_type = attn_type\n",
    "        self.dropout_out = dropout_out\n",
    "        \n",
    "        if emb is not None:\n",
    "            assert emb.size(0) == output_size, 'Invalid embeddings!'\n",
    "            self.embed_size = emb.size(1)            \n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, self.embed_size, padding_idx=0)\n",
    "        self.gru = nn.GRU(self.embed_size, hidden_size, num_layers, dropout=0.3)\n",
    "        self.attention = Attention(hidden_size, context_size, attn_type=attn_type)\n",
    "        self.hid2emb = nn.Linear(hidden_size, self.embed_size)\n",
    "        self.emb2out = nn.Linear(self.embed_size, output_size)\n",
    "        \n",
    "        if dropout_out > 0:\n",
    "            self.out_dropout = nn.Dropout(dropout_out)\n",
    "            \n",
    "        if emb is not None:\n",
    "            self.embedding.weight.data = emb\n",
    "        \n",
    "        if tie_weights:\n",
    "            self.emb2out.weight.data = self.embedding.weight.data\n",
    "            \n",
    "    def forward(self, inp, hidden, context, mask):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(inp).view(1, hidden.size(1), -1)\n",
    "        \n",
    "        # Recurrent layer\n",
    "        gru_out, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        # Attention\n",
    "        attn, output = self.attention(gru_out, context, mask)\n",
    "        \n",
    "        # from hidden to embeddings\n",
    "        output = F.tanh(self.hid2emb(output))\n",
    "        if self.dropout_out > 0:\n",
    "            output = self.out_dropout(output)\n",
    "        \n",
    "        # Classifier\n",
    "        output = self.emb2out(output)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        return output, hidden, attn\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return to_var(torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/attn.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "# dot product attention\n",
    "scores = F.softmax(torch.bmm(hidden_, context_), dim=-1)          # (bs, 1, sl)\n",
    "output = self.ctx2hid(torch.bmm(scores, context.transpose(0, 1))) # (bs, 1, hs)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqAttn(nn.Module):\n",
    "    def __init__(self, input_size, output_size, enc_embed_size, dec_embed_size,\n",
    "                 enc_hidden_size, dec_hidden_size, enc_num_layers, dec_num_layers, \n",
    "                 ndir=2, tie_weights=False, attn_type='mlp', dropout_emb=0, \n",
    "                 dropout_rnn=0, dropout_ctx=0, dropout_out=0, enc_emb=None, dec_emb=None):\n",
    "        \n",
    "        super(Seq2SeqAttn, self).__init__()\n",
    "        self.encoder = EncoderRNN(input_size, enc_embed_size, enc_hidden_size, enc_num_layers, ndir, \n",
    "                                  dropout_emb, dropout_rnn, dropout_ctx, enc_emb)\n",
    "        self.enc2dec = nn.Linear(enc_hidden_size, dec_num_layers * dec_hidden_size, bias=True)\n",
    "        self.decoder = AttnDecoderRNN(dec_embed_size, dec_hidden_size, output_size, enc_hidden_size,\n",
    "                                      dec_num_layers, tie_weights, attn_type, dropout_out, dec_emb)\n",
    "                \n",
    "    def forward(self, inputs, src_lengths, tgt_var=None, teacher_forcing_ratio=0.0):\n",
    "        bs = inputs.size(1)\n",
    "        \n",
    "        # Encoder\n",
    "        hidden = self.encoder.init_hidden(bs)\n",
    "        context, _, mask = self.encoder(inputs, src_lengths, hidden)\n",
    "        \n",
    "        # Encoder 2 Decoder\n",
    "        nl = self.encoder.num_layers\n",
    "        hidden = hidden.view(nl, ndir, bs, -1).permute(0, 2, 1, 3).contiguous().view(nl, bs, -1)\n",
    "        hidden = F.tanh(self.enc2dec(hidden))        \n",
    "        \n",
    "#         # Encoder 2 Decoder\n",
    "#         hidden = F.tanh(self.enc2dec(context.sum(0) / mask.sum(0).unsqueeze(1)))  # (bs, ndir*he) -> (bs, nl * hd)\n",
    "#         nl, hs = self.decoder.num_layers, self.decoder.hidden_size\n",
    "#         hidden = hidden.view(bs, nl, hs).transpose(0, 1).contiguous()\n",
    "\n",
    "        # Decoder\n",
    "        dec_input = to_var(torch.LongTensor([[SOS] * bs]))\n",
    "        dec_outputs, attns = [], []\n",
    "\n",
    "        if (tgt_var is not None):\n",
    "            tgt_len = tgt_var.size(0)\n",
    "        else:\n",
    "            tgt_len = src_lengths[0]\n",
    "            \n",
    "        for i in range(tgt_len):\n",
    "            dec_output, hidden, attn = self.decoder(dec_input, hidden, context, mask)\n",
    "            \n",
    "            dec_outputs += [dec_output]\n",
    "            attns += [attn]\n",
    "            \n",
    "            # next input to decoder\n",
    "            if (tgt_var is not None) and (random.random() < teacher_forcing_ratio):\n",
    "                dec_input = to_var(tgt_var.data[i])\n",
    "            else:\n",
    "                topi = dec_output.data.topk(1, dim=1)[1]\n",
    "                dec_input = to_var(topi.transpose(0, 1))  # (1, bs) -> (bs, 1)\n",
    "            \n",
    "            if torch.sum(dec_input == EOS).data[0] + torch.sum(dec_input == PAD).data[0] == bs:\n",
    "                break\n",
    "                        \n",
    "        return torch.stack(dec_outputs), torch.stack(attns)\n",
    "    \n",
    "    def freeze_embeddings(self):\n",
    "            for p in self.encoder.embedding.parameters():\n",
    "                p.requires_grad = False\n",
    "            for p in self.decoder.embedding.parameters():\n",
    "                p.requires_grad = False\n",
    "                \n",
    "    def unfreeze_embeddings(self):\n",
    "            for p in self.encoder.embedding.parameters():\n",
    "                p.requires_grad = True\n",
    "            for p in self.decoder.embedding.parameters():\n",
    "                p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/attention_paper.png\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train we run the input sentence through the encoder, and keep track of every output and the latest hidden state.\n",
    "\n",
    "Decoder's inputs:\n",
    "- The decoder is given the `<SOS>` token as its first input, and \n",
    "- The last hidden state of the encoder as its first hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher forcing\n",
    "- “Teacher forcing” is the concept of using the real target outputs as each next input, instead of using the decoder’s guess as the next input. \n",
    "- Using teacher forcing causes it to converge faster but when the trained network is exploited, it may exhibit instability.\n",
    "- So, the best strategy is to use teacher forcing **ocassionally**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model_dir, model, val_loss):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    torch.save(model.state_dict(), f'{model_dir}/seq2seq-{val_loss:.2f}.pth')\n",
    "\n",
    "def load_model(model_dir, input_size, enc_emb_sz, dec_emb_size, enc_hidden_size, dec_hidden_size, output_size, num_layers):\n",
    "    model = Seq2SeqAttn(input_size, output_size, enc_emb_sz, dec_emb_size, enc_hidden_size, dec_hidden_size, num_layers, num_layers)\n",
    "    model.load_state_dict(torch.load(f'{model_dir}/seq2seq.pth'))\n",
    "    return model.cuda() if use_cuda else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_dl, valid_dl, optimizer, scheduler, criterion, start=0, num_epochs=10, save_to='.'):\n",
    "    best_val = float('inf')\n",
    "    best_weights = None\n",
    "    plot_losses = []    \n",
    "    for epoch in range(start, start + num_epochs):\n",
    "#         tfr = max(1.0 - 0.1 * epoch, 0) / 2.0 # adjust techer forcing ratio\n",
    "        tfr = 0.0\n",
    "        trn_loss = train_step(model, train_dl, optimizer, criterion, tfr, epoch, start + num_epochs)        \n",
    "        val_loss = validate_step(model, valid_dl, criterion, epoch, start + num_epochs)           \n",
    "        plot_losses.append((trn_loss, val_loss))\n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_weights = model.state_dict().copy()\n",
    "            save_model(model_dir, model, val_loss)\n",
    "            \n",
    "    model.load_state_dict(best_weights)\n",
    "    show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model hyper-parameters\n",
    "enc_hidden_size, dec_hidden_size = 256, 256\n",
    "enc_embed_size, dec_embed_size = 300, 300\n",
    "enc_emb, dec_emb = src_embeddings, tgt_embeddings\n",
    "enc_num_layers, dec_num_layers = 4, 4\n",
    "ndir = 2\n",
    "model_dir = f'models-{enc_hidden_size}-{dec_hidden_size}-{num_layers}'\n",
    "\n",
    "# build the model\n",
    "model = Seq2SeqAttn(len(src_vocab), len(tgt_vocab), enc_embed_size, dec_embed_size,\n",
    "                    enc_hidden_size, dec_hidden_size, num_layers, num_layers, ndir=2,\n",
    "                    tie_weights=True, attn_type='mlp', dropout_emb=0.2, dropout_rnn=.25,\n",
    "                    dropout_ctx=0.2, dropout_out=0.2, enc_emb=enc_emb, dec_emb=dec_emb)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train(model, train_dl, valid_dl, optimizer, scheduler, criterion, start=0, num_epochs=10, save_to=model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = load_model(model_dir, len(src_vocab), \n",
    "                   enc_embed_size, dec_embed_size,\n",
    "                   enc_hidden_size, dec_hidden_size, \n",
    "                   len(tgt_vocab), num_layers)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(model, train_dl, valid_dl, optimizer, scheduler, criterion, start=10, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save_model(model_dir, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate(model, src_var, lengths): \n",
    "    src_len, bs = src_var.size()[:2]\n",
    "    assert bs == 1, print('Batch size in translate must be 1.')\n",
    "    outputs, attentions = model(src_var, lengths)\n",
    "    preds = torch.max(outputs.data.squeeze(1), dim=1)[1]\n",
    "    \n",
    "    translation = []\n",
    "    for pred_id in preds:\n",
    "        translation += [tgt_vocab.itow(pred_id)]\n",
    "        if pred_id == EOS: break\n",
    "    \n",
    "    translation = ' '.join(translation[:-1])  \n",
    "    return translation, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_randomly(model, dl, n=10):\n",
    "    model.encoder.eval()\n",
    "    model.decoder.eval()\n",
    "    \n",
    "    for i, (src, tgt, lengths) in enumerate(dl):\n",
    "        if i >= n: break\n",
    "        print('>', dl.dataset.decode(src.squeeze(1), src_vocab))\n",
    "        print('=', dl.dataset.decode(tgt.squeeze(1), tgt_vocab))\n",
    "        \n",
    "        # translate src sentence\n",
    "        translation, attentions = translate(model, to_var(src, volatile=True), lengths)\n",
    "        if tgt_vocab.lang_name == 'fa':\n",
    "            translation = translation.replace('<UNK>', '<؟>')\n",
    "        print('<', translation)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_dl = DataLoader(valid_ds, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "evaluate_randomly(model, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Improvements (Data, Model, Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Larger corpus\n",
    "- Beam Search\n",
    "- Pre-trained word vectors (`fasttext` or `GloVe`)\n",
    "- Ensemble decoding\n",
    "- Handling rare words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
